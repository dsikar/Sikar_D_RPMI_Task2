\subsubsection{Ford Multi-AV Seasonal Dataset}
The Ford Multi-AV Seasonal Dataset (\cite{agarwal2020ford}) is a multi-sensor dataset collected by a fleet of Ford autonomous vehicles at different days and times during 2017-18. The vehicles were manually driven on an average route of 66 km that included a mix of driving scenarios like city-centres, university campus and suburban neighbourhood, etc. Each vehicle used in this data collection is a Ford Fusion outfitted with an Applanix POS-LV inertial measurement unit (IMU), four HDL-32E Velodyne 3D-lidar scanners, 6 Point Grey 1.3 MP Cameras arranged on the rooftop for 360 degree coverage and 1 Pointgrey 5 MP camera mounted behind the windsheild for forward field of view. We present the seasonal variation in weather, lighting, construction and traffic conditions experienced in dynamic urban environments. This dataset can help design robust algorithms for autonomous vehicles and multi-agent systems. Each log in the dataset is time-stamped and contains raw data from all the sensors, calibration values, pose trajectory, ground truth pose, and 3D maps. All data is available in Rosbag format that can be visualized, modified and applied using the open-source Robot Operating System (ROS).

\subsubsection{Audi dataset}

The Audi dataset (\cite{AUDI}) provides 40,000 frames with semantic segmentation image and point cloud labels, of which more than 12,000 frames also have annotations for 3D bounding boxes. In addition, sensor data (approx. 390,000 frames) for sequences with several loops, recorded in three cities.

Semantic segmentation
The dataset features 41,280 frames with semantic segmentation in 38 categories. Each pixel in an image is given a label describing the type of object it represents, e.g. pedestrian, car, vegetation, etc.

Point cloud segmentation
Point cloud segmentation is produced by fusing semantic pixel information and LiDAR point clouds. Each 3D point is thereby assigned an object type label. This relies on accurate camera-LiDAR registration.

3D bounding boxes
c3D bounding boxes are provided for 12,499 frames. LiDAR points within the field of view of the front camera are labelled with 3D bounding boxes. We annotate 14 classes relevant to driving, e.g. cars, pedestrians, buses, etc.

\subsubsection{KITTI dataset}

Dataset and benchmarks for computer vision research in the context of autonomous driving. The dataset has been recorded in and around the city of Karlsruhe, Germany using the mobile platform AnnieWay (VW station wagon) which has been equipped with several RGB and monochrome cameras, a Velodyne HDL 64 laser scanner as well as an accurate RTK corrected GPS/IMU localization unit. The dataset has been created for computer vision and machine learning research on stereo, optical flow, visual odometry, semantic segmentation, semantic instance segmentation, road segmentation, single image depth prediction, depth map completion, 2D and 3D object detection and object tracking. In addition, several raw data recordings are provided. The datasets are captured by driving around the mid-size city of Karlsruhe, in rural areas and on highways. Up to 15 cars and 30 pedestrians are visible per image (\cite{Geiger2013IJRR}).

\subsubsection{Udacity self-driving datasets}

This resource contains two datasets of interest, containing labeled images with IMU and lidar values (\cite{UDACITY}).



%%  Natural data
We intend to gather traffic image data from the Audi, <the other one> an VW datasets. These are labelled datasets, where a steering angle is provided with every image. We intend to locate segments where rain is present. We expect to require the use of crow sourcing tools such as Amazon Mechanical Turk to locate such segments, as the datasets are quite large.
The expected outcome of this step is at least dozens of sequences containing rain.

%% Synthetic data
We intend to use data augmentation to create synthetic data, with properties that would emulate rain characteristics, such as superimposing rain drops to images, then adding effects such as blurring, reflection and diffusion.  
The expected outcome of this step is at least dozens of sequences containing the augmented data plus metadata indicating the level of each applied effect.