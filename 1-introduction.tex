\section{Introduction}
\label{introduction}
Land vehicles that can drive autonomously, without human intervention, have been increasingly researched in the last 4 decades, harnessing the development in computer hardware, where decreasing size and increasing computational power allow such vehicles to carry mobile computing systems capable of performing the signal and algorithmic processing required to successfully self-drive along a path. Parallel to this development, improvements in one area of research in artificial intelligence (AI) known as machine learning has generated models known as convolutional neural networks (CNNs), successfully applied to the field of computer vision, where current state-of-the-art technology has surpassed human accuracy. Computer vision is a core component of self-driving, which relies heavily on images. 

From the Stanford Cart, a camera-equipped roving robot, with a remote time-sharing computer as its "brain", and "taking about five hours to navigate a 20 meter course" (\cite{3899}) to the current Tesla fleet with billions of accumulated self-driven miles (\cite{AIFSD:2020}), self-driving cars are an increasing presence on public roads.

Automakers such as Tesla, Nissan, Audi, General Motors, BMW, Ford, Honda, Toyota, Mercedes and Volkswagen, and technology companies such as Apple, Google, NVidia and Intel, are currently researching self-driving vehicles (\cite{app10082749}). This involves a number of tasks related to decision-making and resulting motion control: path planning, scene classification, obstacle detection, lane recognition, pedestrian detection, traffic signage detection (including traffic lights). 
Reliability of self-driving cars under changing weather conditions is one important factor, considered since the earlier days of research (\cite{3899}). 

Intel subsidiary MobileEye, NVIDIA and Tesla have developed, and continue to develop dedicated self-driving compute platforms, EyeQ, NVIDIA DRIVE AGX and HW respectively. All three platforms have custom-designed processors, optimised for AI computing. Dedicated inputs receive camera data, and sensor data to measure object proximity, using a combination of radar, sonar and lidar (radio, sound and light waves respectively). These inputs translate into three-dimensional position and orientation information used for path planning.
Tesla has used both MobileEye and NVIDIA platforms, before developing its own (\cite{wiki:TeslaAutopilot}).

Elon Musk, co-founder and CEO of Tesla, with respect to self-driving cars states that  "(...) right now AI and Neural Nets are used really for object recognition (...), identifying objects in still frames and tying it together in a perception path planning layer thereafter. (...) Over time I would expect that it moves really to (...) video in, car steering and pedals out" (\cite{TESLAADE:2019}).  

Based on this scenario, our \textbf{research question} is how do different CNN architectures compare in the rain, the \textbf{purpose of this research} is "to find evidence to inform practice" (\cite{Oates:2006}) on autonomous vehicles transitioning to a CNN and computer-vision-only  solution. The \textbf{product of this research} is an evaluation and the \textbf{intended beneficiary} is primarily the author, hoping this work will create academic and professional opportunities, as well as anyone else researching in the area that may use this work as a stepping stone or starting point.